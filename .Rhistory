n <- 1000
x = rnorm(n)
x <-rnorm(n)
y <- 1 + 2 * x^2 + rnorm(n)
plot(x, y)
mod <- lm(y ~ x)
res <- residuals(mod)
plot(x, res)
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
getwd()
setwd()
install.packages(c("cli", "crayon", "datawizard", "jsonlite", "openssl", "readr", "rlang", "tinytex", "vctrs", "vroom"))
library(tidyverse)
p <- (1:100)/100
i
i <- log(1/p)
ggplot() +
geom_line(aes(x = p, y = i))
ggplot() +
geom_line(aes(x = p, y = p*i))
p0 <- (1:100)/100
i0 <- log(1/p0)
p1 <- 1 - p0
i1 <- log(1/p1)
ggplot() +
geom_line(aes(x = p0, y = p0*i0))
ggplot() +
geom_line(aes(x = p0, y = p0*i0 + p1*i1))
install.packages(c("bit", "bslib", "callr", "data.table", "datawizard", "digest", "DT", "evaluate", "fontawesome", "ggplot2", "ggrepel", "glmnet", "insight", "jsonlite", "knitr", "LiblineaR", "lme4", "lubridate", "magic", "minqa", "modelr", "ndjson", "openssl", "pkgload", "processx", "ps", "qualtRics", "RcppArmadillo", "RcppEigen", "rmarkdown", "sass", "shiny", "stringr", "sys", "vctrs", "xfun", "yaml"))
?textstat_simil
library(quanteda.textstats)
?textstat_simil
library(quanteda.textstats)
?`textstat_simil-class`
library("quanteda")
dfmat <- corpus_subset(data_corpus_inaugural, Year > 2000) %>%
tokens(remove_punct = TRUE) %>%
tokens_remove(stopwords("english")) %>%
dfm()
(tstat1 <- textstat_simil(dfmat, method = "cosine", margin = "documents"))
class(tstat1)
library("quanteda")
##ここできない
require(quanteda.textstats)
require(Matrix)
dfmat_inaug_post1980 <- corpus_subset(data_corpus_inaugural, Year > 1980) %>%
tokens(remove_punct = TRUE) %>%
tokens_wordstem(language = "en") %>%
tokens_remove(stopwords("en")) %>%
dfm()
tstat_obama <- textstat_simil(dfmat_inaug_post1980, dfmat_inaug_post1980[c("2009-Obama","2013-Obama"), ],
margin = "documents", method = "cosine")
as.list(tstat_obama)
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
devtools::install_github("kbenoit/quanteda.dictionaries")
library("quanteda")
corp_uk <- corpus(data_char_ukimmig2010)  # build a new corpus from the texts
summary(corp_uk)
docvars(corp_uk, "Party") <- names(data_char_ukimmig2010)
docvars(corp_uk, "Year") <- 2010
summary(corp_uk)
as.character(data_corpus_inaugural)[2]
summary(data_corpus_inaugural, n = 5)
tokeninfo <- summary(data_corpus_inaugural)
tokeninfo$Year <- docvars(data_corpus_inaugural, "Year")
if (require(ggplot2)) ggplot(data = tokeninfo, aes(x = Year, y = Tokens, group = 1)) +
geom_line() + geom_point() + scale_x_continuous(labels = c(seq(1789, 2017, 12)),
breaks = seq(1789, 2017, 12)) + theme_bw()
corp1 <- corpus(data_corpus_inaugural[1:5])
corp2 <- corpus(data_corpus_inaugural[53:58])
corp3 <- corp1 + corp2
summary(corp3)
summary(corpus_subset(data_corpus_inaugural, Year > 1990))
summary(corpus_subset(data_corpus_inaugural, President == "Adams"))
data_tokens_inaugural <- tokens(data_corpus_inaugural)
kwic(data_tokens_inaugural, pattern = "terror")
kwic(data_tokens_inaugural, pattern = "terror", valuetype = "regex")
kwic(data_tokens_inaugural, pattern = "communist*")
kwic(data_tokens_inaugural, pattern = phrase("United States")) %>%
head()  # show context of the first six occurrences of 'United States'
# inspect the document-level variables
head(docvars(data_corpus_inaugural))
txt <- c(text1 = "This is $10 in 999 different ways,\n up and down; left and right!",
text2 = "@kenbenoit working: on #quanteda 2day\t4ever, http://textasdata.com?page=123.")
tokens(txt)
tokens(txt, remove_numbers = TRUE, remove_punct = TRUE)
tokens(txt, remove_numbers = FALSE, remove_punct = TRUE)
tokens(c("Kurt Vongeut said; only assholes use semi-colons.", "Today is Thursday in Canberra:  It is yesterday in London.",
"En el caso de que no puedas ir con ellos, ¿quieres ir con nosotros?"), what = "sentence")
tokens("New York City is located in the United States.") %>%
tokens_compound(pattern = phrase(c("New York City", "United States")))
corp_inaug_post1990 <- corpus_subset(data_corpus_inaugural, Year > 1990)
# make a dfm
dfmat_inaug_post1990 <- tokens(corp_inaug_post1990) %>%
dfm()
dfmat_inaug_post1990[, 1:5]
# make a dfm, removing stopwords and applying stemming
dfmat_inaug_post1990 <- dfm(dfmat_inaug_post1990, remove = stopwords("english"),
stem = TRUE, remove_punct = TRUE)
dfmat_uk <- tokens(data_char_ukimmig2010, remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
dfm()
dfmat_uk
topfeatures(dfmat_uk, 20)  # 20 most frequent words
set.seed(100)
library("quanteda.textplots")
textplot_wordcloud(dfmat_uk, min_count = 6, random_order = FALSE, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
dfmat_pres <- tail(data_corpus_inaugural, 20) %>%
tokens(remove_punct = TRUE) %>%
tokens_remove(stopwords("en")) %>%
dfm() %>%
dfm_group(groups = Party)
dfm_sort(dfmat_pres)
corp_inaug_post1991 <- corpus_subset(data_corpus_inaugural, Year > 1991)
dict <- dictionary(list(terror = c("terrorism", "terrorists", "threat"),
economy = c("jobs","business", "grow", "work")))
dfmat_inaug_post1991_dict <- tokens(corp_inaug_post1991) %>%
tokens_lookup(dictionary = dict) %>%
dfm()
dfmat_inaug_post1991_dict
require(quanteda.dictionaries)
dictliwc <- dictionary(file = "data/LIWC2001_English.dic", format = "LIWC")
dfmat_inaug_subset <- dfm(tokens(data_corpus_inaugural[52:58]), dictionary = dictliwc)
dfmat_inaug_subset[, 1:10]
##ここできない
require(quanteda.textstats)
require(Matrix)
dfmat_inaug_post1980 <- corpus_subset(data_corpus_inaugural, Year > 1980) %>%
tokens(remove_punct = TRUE) %>%
tokens_wordstem(language = "en") %>%
tokens_remove(stopwords("en")) %>%
dfm()
tstat_obama <- textstat_simil(dfmat_inaug_post1980, dfmat_inaug_post1980[c("2009-Obama","2013-Obama"), ],
margin = "documents", method = "cosine")
as.list(tstat_obama)
install.packages(c("htmltools", "quanteda", "rbibutils", "roxygen2", "testthat"))
install.packages(c("broom", "bslib", "cli", "collapse", "colorspace", "curl", "datawizard", "dbplyr", "digest", "DT", "evaluate", "extrafont", "fansi", "fs", "highr", "htmlwidgets", "httpuv", "isoband", "lubridate", "network", "pkgdown", "purrr", "ragg", "rbibutils", "Rcpp", "RcppArmadillo", "RcppParallel", "RcppTOML", "readODS", "reticulate", "rmarkdown", "Rttf2pt1", "sass", "shiny", "sna", "statmod", "statnet.common", "stringi", "tidyr", "tidytext", "timechange", "tinytex", "tokenizers", "vctrs", "vroom", "xfun", "yaml"))
install.packages(c("collapse", "dplyr", "fontawesome", "forcats", "gargle", "insight", "knitr", "markdown", "RcppTOML", "RCurl", "reticulate", "utf8", "xfun"))
install.packages("list")
library(list)
?ictreg
data(race)
set.seed(1)
# Calculate list experiment difference in means
diff.in.means.results <- ictreg(y ~ 1, data = race,
treat = "treat", J=3, method = "lm")
summary(diff.in.means.results)
View(race)
lm.results <- ictreg(y ~ south + age + male + college, data = race,
treat = "treat", J=3, method = "lm")
summary(lm.results)
plot(lm.results)
data(race)
race.south <- race.nonsouth <- race
race.south[, "south"] <- 1
race.nonsouth[, "south"] <- 0
ml.results.south.nocov <- ictreg(y ~ 1,
data = race[race$south == 1, ], method = "ml", treat = "treat",
J = 3, overdispersed = FALSE, constrained = TRUE)
ml.results.nonsouth.nocov <- ictreg(y ~ 1,
data = race[race$south == 0, ], method = "ml", treat = "treat",
J = 3, overdispersed = FALSE, constrained = TRUE)
avg.pred.south.nocov <- predict(ml.results.south.nocov,
newdata = as.data.frame(matrix(1, 1, 1)), se.fit = TRUE,
avg = TRUE)
avg.pred.nonsouth.nocov <- predict(ml.results.nonsouth.nocov,
newdata = as.data.frame(matrix(1, 1, 1)), se.fit = TRUE,
avg = TRUE)
lm.results <- ictreg(y ~ south + age + male + college,
data = race, treat = "treat", J=3, method = "lm")
# Calculate average predictions for respondents in the
# South and the the North of the US for the lm model,
# replicating the estimates presented in Figure 1, Imai (2010)
avg.pred.south.lm <- predict(lm.results, newdata = race.south,
se.fit = TRUE, avg = TRUE)
avg.pred.nonsouth.lm <- predict(lm.results, newdata = race.nonsouth,
se.fit = TRUE, avg = TRUE)
avg.pred.south.lm
summary(avg.pred.south.lm)
nls.results <- ictreg(y ~ south + age + male + college,
data = race, treat = "treat", J=3, method = "nls")
# Calculate average predictions for respondents in the South
# and the the North of the US for the NLS model, replicating
# the estimates presented in Figure 1, Imai (2010)
avg.pred.nls <- predict(nls.results, newdata = race.south,
newdata.diff = race.nonsouth, se.fit = TRUE, avg = TRUE)
avg.pred.nls
nls.results
avg.pred.nls
avg.pred.nls$fit
avg.pred.nls$se.fit
avg.pred.nls$se.fit[1]
avg.pred.nls$se.fit[1] + 1.96*avg.pred.nls[1]
avg.pred.nls$se.fit[1] + 1.96*avg.pred.nls$se.fit[1]
1.96*avg.pred.nls$se.fit[1]
avg.pred.nls$se.fit[1]
avg.pred.nls$fit[1] + 1.96*avg.pred.nls$se.fit[1]
avg.pred.nls$fit[1]
avg.pred.nls$fit[,1]
avg.pred.nls$fit[1,1]
avg.pred.nls$fit[1,1] + 1.96*avg.pred.nls$se.fit[1]
avg.pred.nls$fit[1,1] - 1.96*avg.pred.nls$se.fit[1]
nls.results <- ictreg(y ~ south + age + male + college,
data = race, treat = "treat", J=3, method = "nls")
# Calculate average predictions for respondents in the South
# and the the North of the US for the NLS model, replicating
# the estimates presented in Figure 1, Imai (2010)
avg.pred.nls <- predict(nls.results, newdata = race.south,
newdata.diff = race.nonsouth, se.fit = TRUE, avg = TRUE, interval = "confidence")
avg.pred.nls
nls.results <- ictreg(y ~ south + age + male + college,
data = race, treat = "treat", J=3, method = "nls")
# Calculate average predictions for respondents in the South
# and the the North of the US for the NLS model, replicating
# the estimates presented in Figure 1, Imai (2010)
avg.pred.nls <- predict(nls.results, newdata = race.south,
newdata.diff = race.nonsouth, se.fit = TRUE, avg = TRUE, interval = c("confidence"))
avg.pred.nls
summary(avg.pred.nls)
avg.pred.nls$se.fit
avg.pred.nls
library(list)
data(race)
race.south <- race.nonsouth <- race
race.south[, "south"] <- 1
race.nonsouth[, "south"] <- 0
nls.results <- ictreg(y ~ south + age + male + college,
data = race, treat = "treat", J=3, method = "nls")
avg.pred.nls <- predict(nls.results, newdata = race.south,
newdata.diff = race.nonsouth, se.fit = TRUE, avg = TRUE)
avg.pred.nls
avg.pred.nls <- predict(nls.results, newdata = race.south,
newdata.diff = race.nonsouth, avg = TRUE,
interval = "confidence")
avg.pred.nls
avg.pred.nls
avg.pred.nls <- predict(nls.results, newdata = race.south,
newdata.diff = race.nonsouth, se.fit = TRUE, avg = TRUE,
interval = "confidence")
avg.pred.nls
data.frame(estimate = avg.pred.nls$fit,
se = avg.pred.nls$se.fit)
as.data.frame(avg.pred.nls$fit)
library(ggplot2)
library(tidyverse)
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr))
as.data.frame(avg.pred.nls$fit) %>%
mutate(type = c("south", "non-south", "difference")) %>%
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr))
as.data.frame(avg.pred.nls$fit) %>%
mutate(type = c("south", "non-south", "difference")) %>%
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr)) +
theme_bw()
as.data.frame(avg.pred.nls$fit) %>%
mutate(type = c("south", "non-south", "difference") %>%
fct_inorder()) %>%
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr)) +
theme_bw()
as.data.frame(avg.pred.nls$fit) %>%
mutate(type = c("south", "non-south", "difference") %>%
fct_inorder()) %>%
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr)) +
geom_vline(aes(yintercept = 0), linetype = "dashed") +
theme_bw()
as.data.frame(avg.pred.nls$fit) %>%
mutate(type = c("south", "non-south", "difference") %>%
fct_inorder()) %>%
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr)) +
geom_hline(aes(yintercept = 0), linetype = "dashed") +
theme_bw()
avg.pred.nls
as.data.frame(avg.pred.nls$fit) %>%
mutate(type = c("south", "non-south", "difference") %>%
fct_inorder()) %>%
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr)) +
geom_hline(aes(yintercept = 0), linetype = "dashed") +
theme_bw() +
labs(x = "", y = "")
as.data.frame(avg.pred.nls$fit) %>%
mutate(type = c("south", "non-south", "difference") %>%
fct_inorder()) %>%
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr)) +
geom_hline(aes(yintercept = 0), linetype = "dashed") +
theme_bw() +
labs(x = "", y = "", title = "NLS with Covariates")
avg.pred.diff.mle <- predict(ml.constrained.results,
newdata = race.south, newdata.diff = race.nonsouth,
se.fit = TRUE, avg = TRUE)
ml.constrained.results <- ictreg(y ~ south + age + male + college,
data = race, treat = "treat", J=3, method = "ml",
overdispersed = FALSE, constrained = TRUE)
avg.pred.diff.mle <- predict(ml.constrained.results,
newdata = race.south, newdata.diff = race.nonsouth,
se.fit = TRUE, avg = TRUE)
avg.pred.diff.mle <- predict(ml.constrained.results,
newdata = race.south, newdata.diff = race.nonsouth,
se.fit = TRUE, avg = TRUE, interval = "confidence")
as.data.frame(avg.pred.diff.mle$fit) %>%
mutate(type = c("south", "non-south", "difference") %>%
fct_inorder()) %>%
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr)) +
geom_hline(aes(yintercept = 0), linetype = "dashed") +
theme_bw() +
labs(x = "", y = "", title = "NLS with Covariates")
as.data.frame(avg.pred.diff.mle$fit) %>%
mutate(type = c("south", "non-south", "difference") %>%
fct_inorder()) %>%
ggplot() +
geom_pointrange(aes(x = type, y = fit, ymin = lwr, ymax = upr)) +
geom_hline(aes(yintercept = 0), linetype = "dashed") +
theme_bw() +
labs(x = "", y = "", title = "ML with Covariates")
install.packages("tidygraph")
install.packages("ggrepel")
install.packages(c("fs", "ggplot2", "httpuv", "igraph", "lubridate", "pdftools", "RcppArmadillo", "readr", "readxl"))
install.packages("data.table")
install.packages(c("cachem", "collapse", "dbplyr", "dtplyr", "fastmap", "Formula", "gh", "haven", "httr", "igraph", "RcppArmadillo", "RcppParallel", "renv", "tidyverse"))
install.packages("conText")
install.packages(c("broom", "gapminder", "openssl", "quanteda.textstats", "tibble", "VGAM"))
install.packages(c("blob", "commonmark", "dbplyr", "gtable", "hms", "htmlwidgets", "insight", "lme4", "ps", "qpdf", "renv", "rlang", "testthat", "tibble", "vctrs"))
install.packages(c("cli", "googlesheets4", "htmltools", "xfun"))
quarto_version()
install.packages(c("collapse", "datawizard", "gargle", "ggplot2", "igraph", "markdown", "plm", "ps", "quanteda", "quanteda.textplots", "RcppArmadillo", "RCurl", "readtext", "renv", "zip", "zoo"))
install.packages(c("broom", "bslib", "countrycode", "curl", "gargle", "googledrive", "googlesheets4", "jsonlite", "knitr", "RcppArmadillo", "rmarkdown", "sys"))
りんご <- 100
みかん <- 150
3*りんご + 2*みかん
install.packages(c("digest", "gargle", "igraph", "RcppArmadillo", "vctrs"))
library(rvest)
library(dplyr)
library(stringr)
library(httr)
url <- https://www.hcdn.gob.ar/proyectos/resultados-buscador.html
page <- read_html(url)
url <- "https://www.hcdn.gob.ar/proyectos/resultados-buscador.html"
page <- read_html(url)
install.packages(c("bslib", "cpp11", "curl", "fastmatch", "fs", "gargle", "ggplot2", "htmltools", "httr", "igraph", "promises", "purrr", "quanteda.textstats", "RcppArmadillo", "rmarkdown", "tinytex", "uuid", "viridis", "xfun"))
library(rvest)
url <- "https://www.hcdn.gob.ar/proyectos/resultados-buscador.html"
library(rvest)
url <- " https://www.hcdn.gob.ar/proyectos/resultados-buscador.html"
url <- "https://www.hcdn.gob.ar/proyectos/resultados-buscador.html"
read_html(url)
url <- "https://www.hcdn.gob.ar/proyectos/buscador2016-99.html"
read_html(url)
url <- "https://www.hcdn.gob.ar/proyectos/buscador2016-99.html"
read_html(url)
install.packages(c("fontawesome", "gtable", "markdown", "MatrixExtra"))
install.packages("bibtex")
library(bibtex)
read.bib()
?read.bib
base.bib <- read.bib(package = "base")
base.bib
base.bib[[1]]
setwd("~/Dropbox/shohei-doi.github.io")
etwd("~/Dropbox/shohei-doi.github.io")
setwd("~/Dropbox/shohei-doi.github.io")
bib <- read.bib("bibliography/reviewed.bib")
bib
?bibentry
bref <- c(
bibentry(
bibtype = "Manual",
title = "boot: Bootstrap R (S-PLUS) Functions",
author = c(
person("Angelo", "Canty", role = "aut",
comment = "S original"),
person(c("Brian", "D."), "Ripley", role = c("aut", "trl", "cre"),
comment = "R port, author of parallel support",
email = "ripley@stats.ox.ac.uk")
),
year = "2012",
note = "R package version 1.3-4",
url = "https://CRAN.R-project.org/package=boot",
key = "boot-package"
),
bibentry(
bibtype = "Book",
title = "Bootstrap Methods and Their Applications",
author = as.person("Anthony C. Davison [aut], David V. Hinkley [aut]"),
year = "1997",
publisher = "Cambridge University Press",
address = "Cambridge",
isbn = "0-521-57391-2",
url = "http://statwww.epfl.ch/davison/BMA/",
key = "boot-book"
)
)
rref <- bibentry(
bibtype = "Manual",
title = "R: A Language and Environment for Statistical Computing",
author = person("R Core Team"),
organization = "R Foundation for Statistical Computing",
address = "Vienna, Austria",
year = 2014,
url = "https://www.R-project.org/")
print(rref)
print(rref, style = "bibtex")
print(rref, style = "citation")
print(rref, style = "html")
print(rref, style = "latex")
print(rref, style = "R")
bib
print(rref)
toBibtex(bref)
print(bref, style = "citation")
bib
bib <- read.bib("bibliography/reviewed.bib")
bib
print(bib)
print(bib, .bibstyle = "MLA")
print(bib, .bibstyle = "MJSS")
print(bib, .bibstyle = "JSS")
getbBibstyle
getbBibstyle()
getBibstyle()
getBibstyle(all = FALSE)
print(bib, style = "latex")
bib
bib[[1]]
bib[[1]][[1]]
bib[[1]][[1]][[1]]
bib$mizuno2020power$mizuno2020power
bib[[1]]
bib[[1]][1]
bib$mizuno2020power$mizuno2021socially
bib$mizuno2020power$mizuno2020power
bib$author
bib$title
